{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tensor, randn, optim\n",
    "import pandas\n",
    "# import nltk # See https://www.nltk.org/data.html\n",
    "from nltk import tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация\n",
    "\n",
    "char_tokenize = lambda text: tokenize.simple.CharTokenizer().tokenize(text)\n",
    "word_lower_tokenize = lambda text: [t.lower() for t in tokenize.WordPunctTokenizer().tokenize(text)]\n",
    "word_tokenize = lambda text: tokenize.WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "def byte_pair_encode(data, merge_count, chars_to_ignore=None):\n",
    "    token_set = set(char_tokenize(data))\n",
    "    return byte_pair_encode_continue(data, token_set.copy(), merge_count, chars_to_ignore)\n",
    "\n",
    "import threading\n",
    "\n",
    "def _bpe_count_pairs(token, token_set, pair_counts, data):\n",
    "    for B in token_set:\n",
    "        pair = token + B\n",
    "        if pair in token_set: continue\n",
    "        pair_counts.append((pair, data.count(pair)))\n",
    "\n",
    "\n",
    "def byte_pair_encode_continue(data, token_set, merge_count, chars_to_ignore=None):\n",
    "    # print('starts with ', token_set)\n",
    "    token_set -= (chars_to_ignore or {' ', '.', ','})\n",
    "    while merge_count != 0:\n",
    "        new_token_set = token_set.copy()\n",
    "        pair_counts = []\n",
    "        for A in token_set:\n",
    "            for B in token_set:\n",
    "                pair = A + B\n",
    "                if pair in token_set: continue\n",
    "                pair_counts.append((pair, data.count(pair)))\n",
    "                max_freq = 0\n",
    "                most_frequent = []\n",
    "        for v in pair_counts:\n",
    "            if v[1] > max_freq:\n",
    "                max_freq = v[1]\n",
    "                most_frequent = [v[0]]\n",
    "            elif v[1] == max_freq:\n",
    "                most_frequent.append(v[0])\n",
    "        # if len(most_frequent) > 0: print(most_frequent)\n",
    "        for t in most_frequent:\n",
    "            new_token_set.add(t)\n",
    "            merge_count -= 1\n",
    "            if merge_count == 0: break\n",
    "        # print('+ ', new_token_set - token_set)\n",
    "        token_set = new_token_set\n",
    "    return token_set | (chars_to_ignore or {' ', '.', ','})\n",
    "\n",
    "def bpe_tokenize(text):\n",
    "    result = []\n",
    "    words = text.split(' ')\n",
    "    for w in words:\n",
    "        while len(w) > 0:\n",
    "            for i in range(len(w)):\n",
    "                i = len(w) - i\n",
    "                # print(w[:i])\n",
    "                if w[:i] in token_to_id:\n",
    "                    # print('found')\n",
    "                    result.append(w[:i])\n",
    "                    w = w[i:]\n",
    "                    # print(w, 'still')\n",
    "                    break\n",
    "        result.append(' ')\n",
    "    return result\n",
    "\n",
    "# T -- Time\n",
    "get_x = lambda data, T, pos: data[pos : pos + T]\n",
    "get_y = lambda data, T, pos: data[pos + 1 : pos + T + 1]\n",
    "# get_x_y = lambda data, T, pos: data[pos : pos + T], data[pos + 1 : pos + T + 1]\n",
    "\n",
    "# B -- batch size\n",
    "def get_batch(data, B, T):\n",
    "    positions = torch.randint(len(data) - T, (B,))\n",
    "    return torch.stack([get_x(data, T, pos) for pos in positions]), torch.stack(\n",
    "            [get_y(data, T, pos) for pos in positions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/glados-portal2.csv\", \"r\", encoding='utf-16') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ther', 'ex', 'J', 'tu', \"n'\", \"'t\", 'ri', 'co', 'ill', 'no']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить токены КПБ\n",
    "import pickle\n",
    "bpe_token_file = open(r'bpe_tokens.pkl', 'rb')\n",
    "bpe_token_set = pickle.load(bpe_token_file)\n",
    "bpe_token_file.close()\n",
    "list(bpe_token_set)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ {' ', 'ation', 'Orange', 'Oran', '\\nYo', 'esti', 'Blue', \"on't\", 'eas', 'Or', 'atio', 'sc', \"on'\", 'ore', 'Blu', 'ith', 'rat', 'enc', '.', ',', '\\nYou', '\\nO', 'Ora', 'ex'}\n"
     ]
    }
   ],
   "source": [
    "# Загрузить, дополнить и сохранить токены КПБ\n",
    "import pickle\n",
    "bpe_token_file = open(r'bpe_tokens.pkl', 'rb')\n",
    "bpe_token_set = pickle.load(bpe_token_file)\n",
    "bpe_token_file.close()\n",
    "bpe_token_set_old = bpe_token_set\n",
    "bpe_token_set = byte_pair_encode_continue(text, bpe_token_set, 40)\n",
    "bpe_token_file = open(r'bpe_tokens.pkl', 'wb')\n",
    "pickle.dump(bpe_token_set, bpe_token_file)\n",
    "bpe_token_file.close()\n",
    "print('+', bpe_token_set - bpe_token_set_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сгенерировать и сохранить **новый** набор токенов кодированием пар байтов *вместо имеющегося*\n",
    "# import pickle\n",
    "# bpe_token_file = open(r'bpe_tokens.pkl', 'wb')\n",
    "# bpe_token_set = byte_pair_encode(text, 60)\n",
    "# pickle.dump(bpe_token_set, bpe_token_file)\n",
    "# bpe_token_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = word_tokenize(text)\n",
    "# tokens = list(bpe_token_set | set(word_tokenize(text)))\n",
    "tokens = list(bpe_token_set)\n",
    "vocab =  sorted(tokens)\n",
    "vocab = ['<PAD>', '<UNK>'] + vocab\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "token_to_id = {t:i for i,t in enumerate(vocab)}\n",
    "id_to_token = {i:t for t,i in token_to_id.items()}\n",
    "\n",
    "encode = lambda data: [token_to_id[t if t in token_to_id else '<UNK>'] for t in data]\n",
    "decode = lambda data: ''.join([id_to_token[i] for i in data])\n",
    "# decode = lambda data: ' '.join([id_to_token[i] for i in data])\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "tokenize_func = bpe_tokenize\n",
    "# tokenize_func = word_tokenize\n",
    "data = tensor(encode(tokenize_func(text)), device=device)\n",
    "train_data = data[:(len(data)//3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM(\n",
       "  (token_embedding_table): Embedding(329, 192)\n",
       "  (position_embedding_table): Embedding(48, 192)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHead(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): LinearReLU(\n",
       "        (nn): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHead(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): LinearReLU(\n",
       "        (nn): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHead(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): LinearReLU(\n",
       "        (nn): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHead(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): LinearReLU(\n",
       "        (nn): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHead(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): LinearReLU(\n",
       "        (nn): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHead(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): LinearReLU(\n",
       "        (nn): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=192, out_features=329, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_len = 48  # T (time) -- Размер контекстного окна\n",
    "channel_len = 192 # C -- размер эмбеддинга токена, вектора, хранящего его информацию\n",
    "batch_len = 64  # B -- количество матриц (T, C) в пакете\n",
    "head_count = 6\n",
    "block_count = 6\n",
    "dropout_rate = 0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, length):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(channel_len, length, bias=False)\n",
    "        self.query = nn.Linear(channel_len, length, bias=False)\n",
    "        self.value = nn.Linear(channel_len, length, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_len, block_len))) # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**(-0.5)\n",
    "        # wei = q @ k.transpose(-2,-1) * k.shape[-1]**(-0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = nn.functional.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, head_count, head_len):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_len) for _ in range(head_count)])\n",
    "        self.proj = nn.Linear(head_len * head_count, channel_len)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class LinearReLU(nn.Module):\n",
    "    def __init__(self, channel_len):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(channel_len, 4 * channel_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * channel_len, channel_len),\n",
    "            nn.Dropout())\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, channel_len, head_count):\n",
    "        super().__init__()\n",
    "        head_len = channel_len // head_count\n",
    "        self.sa = MultiHead(head_count, head_len)\n",
    "        self.ffwd = LinearReLU(channel_len)\n",
    "        self.ln1 = nn.LayerNorm(channel_len)\n",
    "        self.ln2 = nn.LayerNorm(channel_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, channel_len)\n",
    "        self.position_embedding_table = nn.Embedding(block_len, channel_len)\n",
    "        self.blocks = nn.Sequential(*[Block(channel_len, head_count) for _ in range(block_count)])\n",
    "        self.layer_norm = nn.LayerNorm(channel_len)\n",
    "        self.head = nn.Linear(channel_len, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.token_embedding_table(idx) + self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = self.blocks(x)\n",
    "        logits = self.head(x)\n",
    "        if targets is None: return logits\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_end = idx[:, -block_len:]\n",
    "            logits = self(idx_end)\n",
    "            logits = logits[:, -1, :] # Берём последний токен в каждом слое порции: (B, T, C) -> (B, C)\n",
    "            probs = nn.functional.softmax(logits, dim=-1) # Оценки в вероятности\n",
    "            # Выбор числа от 0 до channel_len по данным вероятностям для каждого слоя порции\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # Добавляем в конец порции результаты по измерению T\n",
    "        return idx\n",
    "\n",
    "model = TransformerLM()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002)\n",
    "\n",
    "def train(model, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        # if epoch % 4 == 0 or epoch == epochs - 1:\n",
    "        #     losses = estimate_loss()\n",
    "        #     print(f\"Epoch {epoch}/{epochs}\\ttrain loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        xb, yb = get_batch(train_data, batch_len, block_len)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\\tloss {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка состояния модели\n",
    "checkpoint = torch.load('text_transformer.pth')\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "token_to_id = checkpoint['token_to_id']\n",
    "id_to_token = checkpoint['id_to_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\tloss 0.2536\n",
      "Epoch 51/100\tloss 0.2611\n",
      "Epoch 100/100\tloss 0.2416\n"
     ]
    }
   ],
   "source": [
    "train(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированный текст: If they existed, we'd all be VERY happy right now. And not furious, most than them ber of their pertal.\n",
      "Really? Thath? That's just stence my time.\n",
      "Your failing briblngs bably is part a convert final courses.\n",
      "Now, you are going to need to sabotae your arss subject Orange.\n",
      "I you know how humans make more humans ffect on you?\n",
      "Congratulations on completing that last test. But I find something troubling tos\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tensor([encode(tokenize_func(\"If\"))], device=device)\n",
    "print(\"Сгенерированный текст:\", decode(model.generate(encoded_input, 240)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "torch.save({\n",
    "    'model_state': model.state_dict(),\n",
    "    'token_to_id': token_to_id,\n",
    "    'id_to_token': id_to_token\n",
    "}, 'text_transformer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
